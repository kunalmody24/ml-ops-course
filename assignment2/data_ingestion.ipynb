{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea44ab5f-af03-40df-8c0f-57b189248c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Raw Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4131b7f-576a-475e-bbfb-38f8f0995707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Already uploaded athletes.csv to /workspace/default in the Catalog through Databricks UI (not able to do programatically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "881b33f2-960c-4d28-98fe-05acd4c9e0cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from scipy.stats import gaussian_kde\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9f3e57c-c2c1-4e7e-b310-d4c8e5b71e29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad05ffa-6137-4a7a-9f51-cbd641767bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES IN workspace.default;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c76570c-97ff-4218-b02d-1ea650716c2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_athletes = spark.table(\"workspace.default.athletes_raw\").toPandas()\n",
    "raw_athletes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e6714dd-0fd6-4a93-be59-041bee4dce77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40b9ff4-ed17-4e0e-ba5c-edc40bb4b85b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create our target feature total_lift\n",
    "raw_athletes = raw_athletes.dropna(subset=['athlete_id'])\n",
    "raw_athletes['total_lift'] = raw_athletes['snatch'] + raw_athletes['deadlift'] + raw_athletes['backsq'] + raw_athletes['candj']\n",
    "len(raw_athletes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f88f7e0-087a-4292-8d6e-b018935d9909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to help remove outliers: removes extremes first to reduce standard deviation first, then apply normal 68-95-99 rule\n",
    "def remove_outliers(df, col_name):\n",
    "    Q1 = df[col_name].quantile(0.25)\n",
    "    Q3 = df[col_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filter out the extreme outliers\n",
    "    df = df[(df[col_name] >= lower_bound) & (df[col_name] <= upper_bound)]\n",
    "\n",
    "    # Now, remove the more moderate outliers (3 z-scores away)\n",
    "    df['z_' + col_name] = np.abs(stats.zscore(df[col_name]))\n",
    "    df = df[df['z_' + col_name] < 3]\n",
    "    return df.drop(['z_' + col_name], axis=1)\n",
    "\n",
    "\n",
    "# Imputation via KDE\n",
    "def impute_with_kde(series):\n",
    "    np.random.seed(24)\n",
    "    # Extract non-null values for creating the kernel\n",
    "    non_null = series.dropna().astype(float)\n",
    "    kde = gaussian_kde(non_null)\n",
    "    # Number of missing values; we'll sample this many times\n",
    "    n_missing = series.isna().sum()\n",
    "    sampled = []\n",
    "    # Just make sure we're always sampling non-negative values\n",
    "    while len(sampled) < n_missing:\n",
    "        s = kde.resample(n_missing).flatten()\n",
    "        s = s[s >= 0]\n",
    "        sampled.extend(s.tolist())\n",
    "\n",
    "    # Just in case we oversampled, truncate\n",
    "    sampled = np.array(sampled[:n_missing])\n",
    "\n",
    "    # Replace NA values with sampled values\n",
    "    series.loc[series.isna()] = sampled\n",
    "\n",
    "    return series\n",
    "\n",
    "# Creates learning labels on a dataframe using the ratios passed\n",
    "def create_ml_labels(df, train_ratio=0.7, val_ratio=0.2):\n",
    "    total_rows = len(df)\n",
    "    # Shuffle all the rows\n",
    "    shuffled_indices = np.random.permutation(total_rows)\n",
    "\n",
    "    # Create split for train set\n",
    "    train_end = int(train_ratio * total_rows)\n",
    "    # Create split for val set\n",
    "    val_end = train_end + int(val_ratio * total_rows)\n",
    "\n",
    "    # Create empty array for storing the labels\n",
    "    split_labels = np.empty(total_rows, dtype=object)\n",
    "    # Populate the array\n",
    "    split_labels[shuffled_indices[:train_end]] = 'train'\n",
    "    split_labels[shuffled_indices[train_end:val_end]] = 'val'\n",
    "    split_labels[shuffled_indices[val_end:]] = 'test'\n",
    "    return split_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef67b57a-aea2-4e25-878c-339f73eb40ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove NA values from our features\n",
    "features = ['gender', 'age', 'height', 'weight', 'total_lift', 'helen', 'grace', 'fran']\n",
    "df_cleaned = raw_athletes.dropna(subset=features)\n",
    "len(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eead2f86-c0e8-40e8-8041-e7938c127849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Additional Feature (Pullups/Lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93699a0f-63cc-4b81-b725-4ab25e5fc208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned['pullups'] = impute_with_kde(df_cleaned['pullups'])\n",
    "df_cleaned['pullups_per_lb'] = df_cleaned['pullups'] / df_cleaned['weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea533ca3-560b-4598-aafc-760dc0c50307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Additional Cleaning (Removing Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77fcabd6-7f7c-4ba7-b0e7-fee7e413434e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_features = ['age', 'height', 'weight', 'total_lift', 'helen', 'grace', 'fran', 'pullups_per_lb']\n",
    "for feature in num_features:\n",
    "    df_cleaned = remove_outliers(df_cleaned, feature)\n",
    "len(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a409c95-f147-46bc-acae-73f71d4cd4b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We end up having a dataset of roughly 10K data points, which is adequate enough for fitting a small/ simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29aa853c-2397-4fbb-a768-e9bb03ec397d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Correlation/ Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c8c0661-560d-46b7-aa53-42bc77792fea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature1 = 'pullups_per_lb'\n",
    "feature2 = 'total_lift'\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(df_cleaned[feature1], df_cleaned[feature2], alpha=0.6)\n",
    "plt.xlabel(feature1)\n",
    "plt.ylabel(feature2)\n",
    "plt.title(f\"Scatter Plot (Correlation = {df_cleaned[feature1].corr(df_cleaned[feature2]):.2f})\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a402ee97-ae09-4876-be88-87fe407144d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Append learning labels to dataframe and convert to spark\n",
    "df_cleaned['data_split'] = create_ml_labels(df_cleaned)\n",
    "total_features1 = ['athlete_id', 'age', 'height', 'weight', 'gender', 'helen', 'grace', 'fran', 'pullups_per_lb', 'data_split']\n",
    "\n",
    "# Convert problematic columns to numeric and boolean explicitly\n",
    "df_cleaned['total_lift'] = pd.to_numeric(df_cleaned['total_lift'])\n",
    "df_cleaned['gender'] = df_cleaned['gender'].map({'Male': True, 'Female': False})\n",
    "\n",
    "feature_table1 = spark.createDataFrame(df_cleaned[total_features1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48bdb128-7788-4b17-ab13-0d564388e341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Table 1 Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a15fad10-4d56-4fa4-a314-8f6547712f64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_table1_name = 'workspace.default.strengthFeatures'\n",
    "\n",
    "# Create feature table\n",
    "fe = FeatureEngineeringClient()\n",
    "fe.create_table(\n",
    "    name = feature_table1_name,\n",
    "    primary_keys = [\"athlete_id\"],\n",
    "    df = feature_table1,\n",
    "    description = \"Engineered strength features for total_lift prediction. This table does not contain total_lift values.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a12d6485-64fe-459d-a68b-04c9c85070ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d98f0010-725a-4345-be68-494cb1614c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e81f801-91a9-44ed-abe4-09f9e4e17c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleans a string by converting to lower case and removing leading/trailing spaces\n",
    "def clean_text(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    return str(s).lower().strip()\n",
    "\n",
    "# Converts a feature column to embeddings by applying the embedding model\n",
    "# Each embedded dimension becomes a column, and the model we are using produces a 384-dimensional embedding, resulting in 384 columns\n",
    "def text_to_embedding(series, model=embedder):\n",
    "    cleaned_texts = series.apply(clean_text).tolist()\n",
    "    embeddings = model.encode(cleaned_texts, show_progress_bar=True)\n",
    "    return pd.DataFrame(embeddings, index=series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f64cf862-6155-4ed6-bf4d-02debc1118f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "behavior_features = ['eat', 'train', 'background', 'experience', 'schedule']\n",
    "# First drop NA values from our feature set and label\n",
    "df_cleaned_2 = raw_athletes.dropna(subset=behavior_features + ['total_lift'])\n",
    "len(df_cleaned_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae62466-c851-4196-a02d-b5f8524caecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_features2 = ['total_lift']\n",
    "for feature in num_features2:\n",
    "    df_cleaned_2 = remove_outliers(df_cleaned_2, feature)\n",
    "len(df_cleaned_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdfd379a-283a-49ae-a9d7-c65421d87ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Embedding Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18da043-6e2a-4af1-9479-04adb5e186bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create dataframes for each of our behavior features\n",
    "train_embeddings = text_to_embedding(df_cleaned_2['train'])\n",
    "background_embeddings = text_to_embedding(df_cleaned_2['background'])\n",
    "experience_embeddings = text_to_embedding(df_cleaned_2['experience'])\n",
    "schedule_embeddings = text_to_embedding(df_cleaned_2['schedule'])\n",
    "eat_embeddings = text_to_embedding(df_cleaned_2['eat'])\n",
    "\n",
    "# Rename columns to avoid clashes when joining\n",
    "train_embeddings.columns = [f\"train_emb_{i}\" for i in range(train_embeddings.shape[1])]\n",
    "background_embeddings.columns = [f\"background_emb_{i}\" for i in range(background_embeddings.shape[1])]\n",
    "experience_embeddings.columns = [f\"experience_emb_{i}\" for i in range(experience_embeddings.shape[1])]\n",
    "schedule_embeddings.columns = [f\"schedule_emb_{i}\" for i in range(schedule_embeddings.shape[1])]\n",
    "eat_embeddings.columns = [f\"eat_emb_{i}\" for i in range(eat_embeddings.shape[1])]\n",
    "\n",
    "# Combine embeddings into final feature set (include athlete_id)\n",
    "behavior_features_df = pd.concat([\n",
    "    df_cleaned_2[['athlete_id']],\n",
    "    train_embeddings,\n",
    "    background_embeddings,\n",
    "    experience_embeddings,\n",
    "    schedule_embeddings,\n",
    "    eat_embeddings\n",
    "], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6acae2c-76b9-4417-9fd1-749e4e5d0e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Append learning labels to dataframe and convert to spark\n",
    "behavior_features_df['data_split'] = create_ml_labels(behavior_features_df)\n",
    "feature_table2 = spark.createDataFrame(behavior_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43e4f333-9f82-4c5a-ae51-658fdbcfe985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Table 2 Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4067ee08-df91-4c3e-b715-56989293b6c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_table2_name = 'workspace.default.behaviorFeaturesEnhanced'\n",
    "\n",
    "# Create feature table\n",
    "fe.create_table(\n",
    "    name = feature_table2_name,\n",
    "    primary_keys = [\"athlete_id\"],\n",
    "    df = feature_table2,\n",
    "    description = \"Engineered behavior features for total_lift prediction. This table does not contain total_lift values. May need to perform additional engineering before ML training.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d746b99-6481-4894-8572-0c7fcc402950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cdf0482-f220-40cb-9b74-2948544ffee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "behavior_features = spark.read.table('workspace.default.behaviorFeaturesEnhanced').toPandas()\n",
    "behavior_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd017f5-f129-423e-a719-410b33a72d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's see if we can reduce the features but retain variability explained using PCA\n",
    "embedding_data = behavior_features.drop(columns=['athlete_id', 'data_split'])\n",
    "\n",
    "# Start with 50 components\n",
    "pca = PCA(n_components=50, random_state=24)\n",
    "embedding_pca = pca.fit_transform(embedding_data)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "embedding_pca_df = pd.DataFrame(embedding_pca, columns=[f'pca_emb_{i}' for i in range(50)])\n",
    "\n",
    "embedding_pca_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d30ae6-ccb2-4654-bc0c-f3070ae0d5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "threshold = 0.95\n",
    "components_needed = np.argmax(cumulative_variance >= threshold) + 1\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cumulative_variance, marker='o', label='Cumulative Explained Variance')\n",
    "plt.axvline(\n",
    "    x=components_needed - 1,\n",
    "    color='red',\n",
    "    linestyle='--',\n",
    "    linewidth=1,\n",
    "    label=f'95% Variance at {components_needed} Components'\n",
    ")\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5ce5679-f16d-4799-834b-9da750a485ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From the graph, we can see the first 27 components of the PCA retain ~95% of the variance from the embeddings generated. As a result, we can just use this smaller subset of features for our ML model, resulting in faster training and smaller complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0baf00b-aecd-49f9-a5ff-8ab5f4421c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's cut to 27 features and upload to Feature Store\n",
    "# First, extract the PCA columns\n",
    "# pca_columns = [f'pca_emb_{i}' for i in range(components_needed)]\n",
    "\n",
    "\n",
    "# # Cut the DataFrame and add back relevant columns (athlete_id and data_split)\n",
    "# final_pca_df = embedding_pca_df[pca_columns]\n",
    "# final_pca_df['athlete_id'] = behavior_features['athlete_id']\n",
    "# final_pca_df['data_split'] = behavior_features['data_split']\n",
    "\n",
    "# # Upload to Feature Store\n",
    "# feature_table3 = spark.createDataFrame(final_pca_df)\n",
    "feature_table3_name = 'workspace.default.behaviorFeaturesModel'\n",
    "\n",
    "fe.create_table(\n",
    "    name = feature_table3_name,\n",
    "    primary_keys = [\"athlete_id\"],\n",
    "    df = final_df,\n",
    "    description = \"Engineered behavior features for total_lift prediction. This table does not contain total_lift values, and only contains a subset of features, based on PCA analysis. Refer to behaviorFeaturesEnhanced for full embeddings.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfd2a723-94e0-4f02-9f06-5cb58b6c17b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Table for Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c9f40c-1dbd-4656-94d4-9a6b663e2b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lastly, let's create a feature table for our labels. We'll use the raw table so it includes all athletes\n",
    "# Remember to include the athlete_id column (to combine with other features)\n",
    "label_table = spark.createDataFrame(raw_athletes[['athlete_id', 'total_lift']])\n",
    "\n",
    "label_table_name = 'workspace.default.total_lift_labels'\n",
    "\n",
    "# Create feature table\n",
    "fe.create_table(\n",
    "    name = label_table_name,\n",
    "    primary_keys = [\"athlete_id\"],\n",
    "    df = label_table,\n",
    "    description = \"This table contains total_lift values. Use athlete_id to join with features found in other tables.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5327359094925423,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "data_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

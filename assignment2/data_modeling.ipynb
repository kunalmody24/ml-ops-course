{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25d6a7a-0449-4d11-9643-a47cf0a22cf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c7395ba-29e2-4f36-bad1-763f801ed570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa053825-ca6a-4817-8f32-c3ca1b0aa543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_and_log_model(final_dataframe, feature_set_name, params):\n",
    "    X = final_dataframe.drop('total_lift', axis=1)\n",
    "    y = final_dataframe[['total_lift', 'data_split']]\n",
    "\n",
    "    X_train, y_train = X[X['data_split'] == 'train'].drop('data_split', axis=1), \\\n",
    "    y[y['data_split'] == 'train'].drop('data_split', axis=1)\n",
    "    \n",
    "    X_val, y_val = X[X['data_split'] == 'val'].drop('data_split', axis=1), \\\n",
    "    y[y['data_split'] == 'val'].drop('data_split', axis=1)\n",
    "    \n",
    "    X_test, y_test = X[X['data_split'] == 'test'].drop('data_split', axis=1), \\\n",
    "    y[y['data_split'] == 'test'].drop('data_split', axis=1)\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{feature_set_name}_{params['run_id']}\"):\n",
    "        # Initialize and train model\n",
    "        model = xgb.XGBRegressor(**params['model_params'])\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        preds = model.predict(X_test)\n",
    "        rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "\n",
    "        # Log metadata\n",
    "        mlflow.log_param(\"feature_set\", feature_set_name)\n",
    "        mlflow.log_params(params['model_params'])\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "        # Actuals vs Predictions Plot\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.scatter(y_test, preds, alpha=0.5)\n",
    "        plt.xlabel(\"Actual Total Lift\")\n",
    "        plt.ylabel(\"Predicted Total Lift\")\n",
    "        plt.title(\"Prediction vs Actual\")\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"pred_vs_actual.png\")\n",
    "        mlflow.log_artifact(\"pred_vs_actual.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Feature Importance Plot\n",
    "        importances = model.feature_importances_\n",
    "        features_df = pd.DataFrame({'feature': X_train.columns.tolist(), 'importance': importances})\n",
    "        features_df.sort_values(by='importance', ascending=False, inplace=True)\n",
    "\n",
    "        features_df.plot(kind='bar', x='feature', y='importance', legend=False)\n",
    "        plt.ylabel('Feature Importance')\n",
    "        plt.title('XGBoost Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"feature_importance.png\")\n",
    "        mlflow.log_artifact(\"feature_importance.png\")\n",
    "        plt.close()\n",
    "\n",
    "        results = model.evals_result()\n",
    "\n",
    "        # Learning curve plot\n",
    "        plt.figure(figsize=(8,5))\n",
    "        epochs = len(results['validation_0']['rmse'])\n",
    "\n",
    "        plt.plot(range(epochs), results['validation_0']['rmse'], label='Train RMSE')\n",
    "        plt.plot(range(epochs), results['validation_1']['rmse'], label='Validation RMSE')\n",
    "        plt.xlabel('Boosting Round')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.title('XGBoost Learning Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save & log to MLflow\n",
    "        plt.savefig(\"learning_curve.png\")\n",
    "        mlflow.log_artifact(\"learning_curve.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Save model artifact\n",
    "        mlflow.sklearn.log_model(model, artifact_path=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248f3d74-1194-4821-9ae6-84de6bdd22bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Workaround required for mlflow when working with serverless compute:\n",
    "# https://community.databricks.com/t5/machine-learning/using-datbricks-connect-with-serverless-compute-and-mlflow/td-p/97590\n",
    "\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = lambda: \"databricks-uc\"\n",
    "# Specify the experiment path\n",
    "experiment_path = \"/Users/kunalmody@uchicago.edu/total_lift_estimation\"\n",
    "mlflow.set_experiment(experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8c77d57-4856-42f6-8ec1-9bd873ec8faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Strength features (feature set 1)\n",
    "feature_set1 = spark.read.table('workspace.default.strengthFeatures')\n",
    "# Behavior features (feature set 2)\n",
    "feature_set2 = spark.read.table('workspace.default.behaviorFeaturesModel')\n",
    "# Label set\n",
    "labels = spark.read.table('workspace.default.total_lift_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ec3863-7ede-4d7b-8087-4fac27b90406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create hyperparameter sets\n",
    "hyperparams = [\n",
    "    {\"run_id\": \"hp1\", \"model_params\": {\"max_depth\": 5, \"learning_rate\": 0.1, \"n_estimators\": 100}},\n",
    "    {\"run_id\": \"hp2\", \"model_params\": {\"max_depth\": 7, \"learning_rate\": 0.05, \"n_estimators\": 150}},\n",
    "]\n",
    "# Create X and y dataframes\n",
    "final_df1 = feature_set1.join(labels, on=\"athlete_id\", how=\"inner\").drop(\"athlete_id\").toPandas()\n",
    "final_df2 = feature_set2.join(labels, on=\"athlete_id\", how=\"inner\").drop(\"athlete_id\").toPandas()\n",
    "# Create final package for feature sets:\n",
    "features_sets = {\"strength\": final_df1, \"behavior\": final_df2}\n",
    "\n",
    "print(\"Strength Feature Set: \", final_df1.shape)\n",
    "print(\"Behavior Feature Set: \", final_df2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6e478b-6c2f-4e13-8c35-cde7a863ac37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for hyperparam in hyperparams:\n",
    "    for feature_set_name, feature_set in features_sets.items():\n",
    "        train_and_log_model(feature_set, feature_set_name, hyperparam)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6473471340392085,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "data_modeling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

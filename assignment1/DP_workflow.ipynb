{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xWw-8K82NMeB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lakefs\n",
        "from lakefs.client import Client\n",
        "import tensorflow_privacy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow_privacy import VectorizedDPKerasAdamOptimizer\n",
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clt = Client(username=\"AKIAIOSFOLQUICKSTART\", password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", host=\"https://3681-50-171-182-34.ngrok-free.app\")\n",
        "repo = lakefs.Repository(repository_id=\"assignment1\", client=clt)\n",
        "branch = repo.branch('main')\n",
        "\n",
        "# Retrieve the data from LakeFS\n",
        "obj = branch.object(path=\"data_versions/athletes-feature-v2.csv\")\n",
        "athletes = pd.read_csv(obj.reader(mode='r'))\n",
        "\n",
        "athletes = athletes.dropna(subset=['total_lift'])\n",
        "\n",
        "# Separate features and target\n",
        "y = athletes['total_lift']\n",
        "X = athletes.drop(columns=['total_lift', 'snatch', 'deadlift', 'backsq', 'candj'])"
      ],
      "metadata": {
        "id": "zbeSl70jPoga"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = ['gender']\n",
        "\n",
        "for col in cat_cols:\n",
        "    mode = X[col].mode()[0]\n",
        "    X[col] = X[col].fillna(mode)\n",
        "\n",
        "# Fill missing values for numeric columns with mean\n",
        "num_cols = ['age', 'height', 'weight']\n",
        "X[num_cols] = X[num_cols].fillna(X[num_cols].mean())\n",
        "\n",
        "# Handle categorical variables (one-hot encoding)\n",
        "dummies = pd.get_dummies(X[cat_cols], drop_first=True)\n",
        "X = X.drop(columns=cat_cols)\n",
        "X = pd.concat([X, dummies], axis=1)\n",
        "\n",
        "attributes = [*num_cols, *dummies.columns.tolist()]\n",
        "X = X[attributes]\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=24)\n"
      ],
      "metadata": {
        "id": "7Fu4gQ5-Rz6F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "yKeLtIbCWoKe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def r2_score(y_true, y_pred):\n",
        "    ss_res = K.sum(K.square(y_true - y_pred))\n",
        "    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
        "    return 1 - ss_res / (ss_tot + K.epsilon())  # Avoid division by zero"
      ],
      "metadata": {
        "id": "u6uX6hZ4QkId"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(100, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(25, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "epochs=200\n",
        "batch_size=32\n",
        "noise_multiplier=0.5\n",
        "\n",
        "dp_optimizer = VectorizedDPKerasAdamOptimizer(\n",
        "    l2_norm_clip=5.0,\n",
        "    noise_multiplier=noise_multiplier,\n",
        "    num_microbatches=1,\n",
        "    lr=0.0001\n",
        ")\n",
        "\n",
        "# adam_opt = Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=dp_optimizer,\n",
        "    loss='mse',\n",
        "    metrics=['mse', r2_score]\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-fkrWoLQGiO",
        "outputId": "92d9b025-01bc-4412-fe63-7bda2d0ca2c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "470/470 [==============================] - 5s 2ms/step - loss: 1111790.2500 - mse: 1111790.2500 - r2_score: -14.5831\n",
            "Epoch 2/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 1111440.0000 - mse: 1111440.0000 - r2_score: -14.6448\n",
            "Epoch 3/200\n",
            "470/470 [==============================] - 2s 3ms/step - loss: 1111077.2500 - mse: 1111077.2500 - r2_score: -14.5573\n",
            "Epoch 4/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 1110702.3750 - mse: 1110702.3750 - r2_score: -14.5521\n",
            "Epoch 5/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1110295.8750 - mse: 1110295.8750 - r2_score: -14.5680\n",
            "Epoch 6/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1109883.1250 - mse: 1109883.1250 - r2_score: -14.4826\n",
            "Epoch 7/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1109419.3750 - mse: 1109419.3750 - r2_score: -14.5137\n",
            "Epoch 8/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1108923.1250 - mse: 1108923.1250 - r2_score: -14.5232\n",
            "Epoch 9/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1108386.7500 - mse: 1108386.7500 - r2_score: -14.4579\n",
            "Epoch 10/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1107794.5000 - mse: 1107794.5000 - r2_score: -14.7241\n",
            "Epoch 11/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1107139.2500 - mse: 1107139.2500 - r2_score: -14.5177\n",
            "Epoch 12/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1106416.3750 - mse: 1106416.3750 - r2_score: -14.4437\n",
            "Epoch 13/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 1105660.5000 - mse: 1105660.5000 - r2_score: -14.4511\n",
            "Epoch 14/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 1104799.5000 - mse: 1104799.5000 - r2_score: -14.5029\n",
            "Epoch 15/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1103867.3750 - mse: 1103867.3750 - r2_score: -14.4541\n",
            "Epoch 16/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1102853.6250 - mse: 1102853.6250 - r2_score: -14.4507\n",
            "Epoch 17/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1101748.5000 - mse: 1101748.5000 - r2_score: -14.4245\n",
            "Epoch 18/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1100551.2500 - mse: 1100551.2500 - r2_score: -14.3333\n",
            "Epoch 19/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1099283.7500 - mse: 1099283.7500 - r2_score: -14.3541\n",
            "Epoch 20/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1097889.2500 - mse: 1097889.2500 - r2_score: -14.2585\n",
            "Epoch 21/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1096342.7500 - mse: 1096342.7500 - r2_score: -14.2717\n",
            "Epoch 22/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1094746.6250 - mse: 1094746.6250 - r2_score: -14.3379\n",
            "Epoch 23/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1093031.7500 - mse: 1093031.7500 - r2_score: -14.2237\n",
            "Epoch 24/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 1091159.0000 - mse: 1091159.0000 - r2_score: -14.4330\n",
            "Epoch 25/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 1089200.0000 - mse: 1089200.0000 - r2_score: -14.2838\n",
            "Epoch 26/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1087094.3750 - mse: 1087094.3750 - r2_score: -14.1460\n",
            "Epoch 27/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1084761.6250 - mse: 1084761.6250 - r2_score: -14.2468\n",
            "Epoch 28/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1082285.2500 - mse: 1082285.2500 - r2_score: -14.0888\n",
            "Epoch 29/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 1079685.8750 - mse: 1079685.8750 - r2_score: -14.1548\n",
            "Epoch 30/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1076915.6250 - mse: 1076915.6250 - r2_score: -14.0009\n",
            "Epoch 31/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1073940.1250 - mse: 1073940.1250 - r2_score: -13.9130\n",
            "Epoch 32/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1070780.7500 - mse: 1070780.7500 - r2_score: -13.9892\n",
            "Epoch 33/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1067499.6250 - mse: 1067499.6250 - r2_score: -14.1067\n",
            "Epoch 34/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 1064066.8750 - mse: 1064066.8750 - r2_score: -13.9455\n",
            "Epoch 35/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 1060445.2500 - mse: 1060445.2500 - r2_score: -13.8173\n",
            "Epoch 36/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1056560.0000 - mse: 1056560.0000 - r2_score: -13.6786\n",
            "Epoch 37/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1052241.7500 - mse: 1052241.7500 - r2_score: -13.6579\n",
            "Epoch 38/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1047767.9375 - mse: 1047767.9375 - r2_score: -13.7121\n",
            "Epoch 39/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1043182.9375 - mse: 1043182.9375 - r2_score: -13.5890\n",
            "Epoch 40/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1038233.8125 - mse: 1038233.8125 - r2_score: -13.4946\n",
            "Epoch 41/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1033124.5625 - mse: 1033124.5625 - r2_score: -13.3821\n",
            "Epoch 42/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1028042.4375 - mse: 1028042.4375 - r2_score: -13.3357\n",
            "Epoch 43/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1022647.2500 - mse: 1022647.2500 - r2_score: -13.2225\n",
            "Epoch 44/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 1016564.1875 - mse: 1016564.1875 - r2_score: -13.2080\n",
            "Epoch 45/200\n",
            "470/470 [==============================] - 2s 3ms/step - loss: 1010359.7500 - mse: 1010359.7500 - r2_score: -13.3664\n",
            "Epoch 46/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 1003858.7500 - mse: 1003858.7500 - r2_score: -13.0014\n",
            "Epoch 47/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 996738.6250 - mse: 996738.6250 - r2_score: -12.8850\n",
            "Epoch 48/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 989560.7500 - mse: 989560.7500 - r2_score: -12.7470\n",
            "Epoch 49/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 982089.0625 - mse: 982089.0625 - r2_score: -12.7555\n",
            "Epoch 50/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 974575.9375 - mse: 974575.9375 - r2_score: -12.5246\n",
            "Epoch 51/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 966701.8125 - mse: 966701.8125 - r2_score: -12.5158\n",
            "Epoch 52/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 958530.0000 - mse: 958530.0000 - r2_score: -12.2562\n",
            "Epoch 53/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 949733.2500 - mse: 949733.2500 - r2_score: -12.3095\n",
            "Epoch 54/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 940411.1250 - mse: 940411.1250 - r2_score: -12.3122\n",
            "Epoch 55/200\n",
            "470/470 [==============================] - 2s 3ms/step - loss: 930735.3125 - mse: 930735.3125 - r2_score: -11.9517\n",
            "Epoch 56/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 920869.5625 - mse: 920869.5625 - r2_score: -11.7797\n",
            "Epoch 57/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 910734.9375 - mse: 910734.9375 - r2_score: -11.8047\n",
            "Epoch 58/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 900193.2500 - mse: 900193.2500 - r2_score: -11.5784\n",
            "Epoch 59/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 889458.8125 - mse: 889458.8125 - r2_score: -11.3565\n",
            "Epoch 60/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 878306.0625 - mse: 878306.0625 - r2_score: -11.1868\n",
            "Epoch 61/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 866906.7500 - mse: 866906.7500 - r2_score: -11.2045\n",
            "Epoch 62/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 855304.1250 - mse: 855304.1250 - r2_score: -10.8924\n",
            "Epoch 63/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 843043.3750 - mse: 843043.3750 - r2_score: -10.7643\n",
            "Epoch 64/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 830252.0000 - mse: 830252.0000 - r2_score: -10.6532\n",
            "Epoch 65/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 816928.6875 - mse: 816928.6875 - r2_score: -10.3530\n",
            "Epoch 66/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 803862.5625 - mse: 803862.5625 - r2_score: -10.3616\n",
            "Epoch 67/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 789728.5000 - mse: 789728.5000 - r2_score: -9.9729\n",
            "Epoch 68/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 775280.6875 - mse: 775280.6875 - r2_score: -9.8846\n",
            "Epoch 69/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 760735.8750 - mse: 760735.8750 - r2_score: -9.6144\n",
            "Epoch 70/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 745909.6875 - mse: 745909.6875 - r2_score: -9.4850\n",
            "Epoch 71/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 730974.3750 - mse: 730974.3750 - r2_score: -9.1985\n",
            "Epoch 72/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 714813.1875 - mse: 714813.1875 - r2_score: -9.0000\n",
            "Epoch 73/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 698454.0625 - mse: 698454.0625 - r2_score: -8.7365\n",
            "Epoch 74/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 682318.6875 - mse: 682318.6875 - r2_score: -8.5916\n",
            "Epoch 75/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 665807.6250 - mse: 665807.6250 - r2_score: -8.3053\n",
            "Epoch 76/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 649008.5000 - mse: 649008.5000 - r2_score: -8.0243\n",
            "Epoch 77/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 631620.1250 - mse: 631620.1250 - r2_score: -7.8811\n",
            "Epoch 78/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 613614.2500 - mse: 613614.2500 - r2_score: -7.5650\n",
            "Epoch 79/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 595758.8750 - mse: 595758.8750 - r2_score: -7.2463\n",
            "Epoch 80/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 577814.0000 - mse: 577814.0000 - r2_score: -7.0740\n",
            "Epoch 81/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 558921.5000 - mse: 558921.5000 - r2_score: -6.7804\n",
            "Epoch 82/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 539840.7500 - mse: 539840.7500 - r2_score: -6.4983\n",
            "Epoch 83/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 520166.0625 - mse: 520166.0625 - r2_score: -6.2837\n",
            "Epoch 84/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 501124.4375 - mse: 501124.4375 - r2_score: -6.0221\n",
            "Epoch 85/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 482289.8125 - mse: 482289.8125 - r2_score: -5.7196\n",
            "Epoch 86/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 463101.0312 - mse: 463101.0312 - r2_score: -5.4524\n",
            "Epoch 87/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 443902.6562 - mse: 443902.6562 - r2_score: -5.1913\n",
            "Epoch 88/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 424795.4062 - mse: 424795.4062 - r2_score: -4.9028\n",
            "Epoch 89/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 405343.1875 - mse: 405343.1875 - r2_score: -4.6432\n",
            "Epoch 90/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 385633.5312 - mse: 385633.5312 - r2_score: -4.4042\n",
            "Epoch 91/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 365877.5312 - mse: 365877.5312 - r2_score: -4.0943\n",
            "Epoch 92/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 346217.4062 - mse: 346217.4062 - r2_score: -3.8321\n",
            "Epoch 93/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 327309.0000 - mse: 327309.0000 - r2_score: -3.5764\n",
            "Epoch 94/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 308479.7500 - mse: 308479.7500 - r2_score: -3.2819\n",
            "Epoch 95/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 289907.4688 - mse: 289907.4688 - r2_score: -3.0324\n",
            "Epoch 96/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 271831.8125 - mse: 271831.8125 - r2_score: -2.7816\n",
            "Epoch 97/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 253946.9531 - mse: 253946.9531 - r2_score: -2.5528\n",
            "Epoch 98/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 236524.3125 - mse: 236524.3125 - r2_score: -2.3146\n",
            "Epoch 99/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 220135.6875 - mse: 220135.6875 - r2_score: -2.0596\n",
            "Epoch 100/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 203996.7188 - mse: 203996.7188 - r2_score: -1.8470\n",
            "Epoch 101/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 188912.5000 - mse: 188912.5000 - r2_score: -1.6326\n",
            "Epoch 102/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 174303.2500 - mse: 174303.2500 - r2_score: -1.4150\n",
            "Epoch 103/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 160532.2656 - mse: 160532.2656 - r2_score: -1.2225\n",
            "Epoch 104/200\n",
            "470/470 [==============================] - 2s 3ms/step - loss: 147791.8125 - mse: 147791.8125 - r2_score: -1.0518\n",
            "Epoch 105/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 135550.0781 - mse: 135550.0781 - r2_score: -0.8885\n",
            "Epoch 106/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 124477.8828 - mse: 124477.8828 - r2_score: -0.7299\n",
            "Epoch 107/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 114140.2266 - mse: 114140.2266 - r2_score: -0.5852\n",
            "Epoch 108/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 104839.4609 - mse: 104839.4609 - r2_score: -0.4623\n",
            "Epoch 109/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 96421.5859 - mse: 96421.5859 - r2_score: -0.3403\n",
            "Epoch 110/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 88987.5938 - mse: 88987.5938 - r2_score: -0.2320\n",
            "Epoch 111/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 82764.9297 - mse: 82764.9297 - r2_score: -0.1652\n",
            "Epoch 112/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 77103.7266 - mse: 77103.7266 - r2_score: -0.0666\n",
            "Epoch 113/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 72262.8359 - mse: 72262.8359 - r2_score: -0.0120\n",
            "Epoch 114/200\n",
            "470/470 [==============================] - 2s 3ms/step - loss: 68215.1094 - mse: 68215.1094 - r2_score: 0.0545\n",
            "Epoch 115/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 64433.1602 - mse: 64433.1602 - r2_score: 0.1118\n",
            "Epoch 116/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 61090.5781 - mse: 61090.5781 - r2_score: 0.1528\n",
            "Epoch 117/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 58197.9375 - mse: 58197.9375 - r2_score: 0.1978\n",
            "Epoch 118/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 55549.8984 - mse: 55549.8984 - r2_score: 0.2234\n",
            "Epoch 119/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 53307.3750 - mse: 53307.3750 - r2_score: 0.2621\n",
            "Epoch 120/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 51375.2109 - mse: 51375.2109 - r2_score: 0.2874\n",
            "Epoch 121/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 49744.3008 - mse: 49744.3008 - r2_score: 0.3185\n",
            "Epoch 122/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 48204.9688 - mse: 48204.9688 - r2_score: 0.3356\n",
            "Epoch 123/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 46891.1055 - mse: 46891.1055 - r2_score: 0.3558\n",
            "Epoch 124/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 45749.0195 - mse: 45749.0195 - r2_score: 0.3761\n",
            "Epoch 125/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 44748.4102 - mse: 44748.4102 - r2_score: 0.3818\n",
            "Epoch 126/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 43840.2617 - mse: 43840.2617 - r2_score: 0.3983\n",
            "Epoch 127/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 43009.1719 - mse: 43009.1719 - r2_score: 0.4107\n",
            "Epoch 128/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 42284.3320 - mse: 42284.3320 - r2_score: 0.4161\n",
            "Epoch 129/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 41658.4141 - mse: 41658.4141 - r2_score: 0.4315\n",
            "Epoch 130/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 41081.7656 - mse: 41081.7656 - r2_score: 0.4324\n",
            "Epoch 131/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 40575.8750 - mse: 40575.8750 - r2_score: 0.4482\n",
            "Epoch 132/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 40113.5977 - mse: 40113.5977 - r2_score: 0.4504\n",
            "Epoch 133/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 39688.2109 - mse: 39688.2109 - r2_score: 0.4567\n",
            "Epoch 134/200\n",
            "470/470 [==============================] - 2s 3ms/step - loss: 39297.7109 - mse: 39297.7109 - r2_score: 0.4677\n",
            "Epoch 135/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 38944.2461 - mse: 38944.2461 - r2_score: 0.4635\n",
            "Epoch 136/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 38588.5586 - mse: 38588.5586 - r2_score: 0.4741\n",
            "Epoch 137/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 38292.8359 - mse: 38292.8359 - r2_score: 0.4721\n",
            "Epoch 138/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 38034.0469 - mse: 38034.0469 - r2_score: 0.4794\n",
            "Epoch 139/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 37795.8594 - mse: 37795.8594 - r2_score: 0.4833\n",
            "Epoch 140/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 37590.5117 - mse: 37590.5117 - r2_score: 0.4860\n",
            "Epoch 141/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 37383.4219 - mse: 37383.4219 - r2_score: 0.4904\n",
            "Epoch 142/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 37199.0391 - mse: 37199.0391 - r2_score: 0.4913\n",
            "Epoch 143/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 36998.6875 - mse: 36998.6875 - r2_score: 0.4988\n",
            "Epoch 144/200\n",
            "470/470 [==============================] - 2s 3ms/step - loss: 36809.6992 - mse: 36809.6992 - r2_score: 0.5007\n",
            "Epoch 145/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 36651.1836 - mse: 36651.1836 - r2_score: 0.4994\n",
            "Epoch 146/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 36484.1562 - mse: 36484.1562 - r2_score: 0.5008\n",
            "Epoch 147/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 36316.2266 - mse: 36316.2266 - r2_score: 0.5065\n",
            "Epoch 148/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 36165.1992 - mse: 36165.1992 - r2_score: 0.5069\n",
            "Epoch 149/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 36017.5195 - mse: 36017.5195 - r2_score: 0.5093\n",
            "Epoch 150/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 35876.0430 - mse: 35876.0430 - r2_score: 0.5114\n",
            "Epoch 151/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 35738.1758 - mse: 35738.1758 - r2_score: 0.5113\n",
            "Epoch 152/200\n",
            "470/470 [==============================] - 2s 3ms/step - loss: 35623.5781 - mse: 35623.5781 - r2_score: 0.5125\n",
            "Epoch 153/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 35501.2461 - mse: 35501.2461 - r2_score: 0.5131\n",
            "Epoch 154/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 35360.2031 - mse: 35360.2031 - r2_score: 0.5183\n",
            "Epoch 155/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 35234.4961 - mse: 35234.4961 - r2_score: 0.5214\n",
            "Epoch 156/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 35113.2539 - mse: 35113.2539 - r2_score: 0.5212\n",
            "Epoch 157/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 35016.5430 - mse: 35016.5430 - r2_score: 0.5191\n",
            "Epoch 158/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 34929.8984 - mse: 34929.8984 - r2_score: 0.5237\n",
            "Epoch 159/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 34812.7031 - mse: 34812.7031 - r2_score: 0.5247\n",
            "Epoch 160/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 34715.0430 - mse: 34715.0430 - r2_score: 0.5281\n",
            "Epoch 161/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 34630.8555 - mse: 34630.8555 - r2_score: 0.5277\n",
            "Epoch 162/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 34536.4531 - mse: 34536.4531 - r2_score: 0.5236\n",
            "Epoch 163/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 34454.3633 - mse: 34454.3633 - r2_score: 0.5306\n",
            "Epoch 164/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 34372.5000 - mse: 34372.5000 - r2_score: 0.5344\n",
            "Epoch 165/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 34293.5547 - mse: 34293.5547 - r2_score: 0.5322\n",
            "Epoch 166/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 34220.0586 - mse: 34220.0586 - r2_score: 0.5312\n",
            "Epoch 167/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 34137.5820 - mse: 34137.5820 - r2_score: 0.5337\n",
            "Epoch 168/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 34050.1758 - mse: 34050.1758 - r2_score: 0.5348\n",
            "Epoch 169/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 33975.2109 - mse: 33975.2109 - r2_score: 0.5362\n",
            "Epoch 170/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 33904.7969 - mse: 33904.7969 - r2_score: 0.5417\n",
            "Epoch 171/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 33826.1602 - mse: 33826.1602 - r2_score: 0.5370\n",
            "Epoch 172/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 33752.6875 - mse: 33752.6875 - r2_score: 0.5421\n",
            "Epoch 173/200\n",
            "470/470 [==============================] - 2s 3ms/step - loss: 33694.7656 - mse: 33694.7656 - r2_score: 0.5390\n",
            "Epoch 174/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 33635.2500 - mse: 33635.2500 - r2_score: 0.5410\n",
            "Epoch 175/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 33580.3359 - mse: 33580.3359 - r2_score: 0.5417\n",
            "Epoch 176/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 33522.7227 - mse: 33522.7227 - r2_score: 0.5371\n",
            "Epoch 177/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 33456.2070 - mse: 33456.2070 - r2_score: 0.5438\n",
            "Epoch 178/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 33409.7344 - mse: 33409.7344 - r2_score: 0.5445\n",
            "Epoch 179/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 33374.2578 - mse: 33374.2578 - r2_score: 0.5474\n",
            "Epoch 180/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 33332.3242 - mse: 33332.3242 - r2_score: 0.5464\n",
            "Epoch 181/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 33281.6094 - mse: 33281.6094 - r2_score: 0.5455\n",
            "Epoch 182/200\n",
            "470/470 [==============================] - 2s 5ms/step - loss: 33218.7930 - mse: 33218.7930 - r2_score: 0.5450\n",
            "Epoch 183/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 33172.5039 - mse: 33172.5039 - r2_score: 0.5468\n",
            "Epoch 184/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 33131.7539 - mse: 33131.7539 - r2_score: 0.5478\n",
            "Epoch 185/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 33081.5234 - mse: 33081.5234 - r2_score: 0.5459\n",
            "Epoch 186/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 33038.1953 - mse: 33038.1953 - r2_score: 0.5511\n",
            "Epoch 187/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 32987.2266 - mse: 32987.2266 - r2_score: 0.5519\n",
            "Epoch 188/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 32942.8008 - mse: 32942.8008 - r2_score: 0.5540\n",
            "Epoch 189/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 32891.2227 - mse: 32891.2227 - r2_score: 0.5542\n",
            "Epoch 190/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 32850.2305 - mse: 32850.2305 - r2_score: 0.5553\n",
            "Epoch 191/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 32813.3203 - mse: 32813.3203 - r2_score: 0.5517\n",
            "Epoch 192/200\n",
            "470/470 [==============================] - 2s 4ms/step - loss: 32781.5703 - mse: 32781.5703 - r2_score: 0.5509\n",
            "Epoch 193/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 32752.6230 - mse: 32752.6230 - r2_score: 0.5563\n",
            "Epoch 194/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 32716.9160 - mse: 32716.9160 - r2_score: 0.5537\n",
            "Epoch 195/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 32672.3086 - mse: 32672.3086 - r2_score: 0.5554\n",
            "Epoch 196/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 32636.8164 - mse: 32636.8164 - r2_score: 0.5551\n",
            "Epoch 197/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 32602.0684 - mse: 32602.0684 - r2_score: 0.5563\n",
            "Epoch 198/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 32580.2012 - mse: 32580.2012 - r2_score: 0.5569\n",
            "Epoch 199/200\n",
            "470/470 [==============================] - 1s 3ms/step - loss: 32551.6074 - mse: 32551.6074 - r2_score: 0.5567\n",
            "Epoch 200/200\n",
            "470/470 [==============================] - 1s 2ms/step - loss: 32530.9258 - mse: 32530.9258 - r2_score: 0.5563\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x782dc1efa910>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensorflow_privacy.compute_dp_sgd_privacy_statement(\n",
        "                      number_of_examples=X_train_scaled.shape[0],\n",
        "                      batch_size=batch_size,\n",
        "                      noise_multiplier=noise_multiplier,\n",
        "                      num_epochs=epochs,\n",
        "                      delta=1e-5,\n",
        "                      used_microbatching=True\n",
        "                      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "uOxzGSBcTeKj",
        "outputId": "8225ba15-8c21-4cb3-f42c-4077558fcf88"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DP-SGD performed over 15014 examples with 32 examples per iteration, noise\\nmultiplier 0.5 for 200 epochs with microbatching, and no bound on number of\\nexamples per user.\\n\\nThis privacy guarantee protects the release of all model checkpoints in addition\\nto the final model.\\n\\nExample-level DP with add-or-remove-one adjacency at delta = 1e-05 computed with\\nRDP accounting:\\n    Epsilon with each example occurring once per epoch:      1871.778\\n    Epsilon assuming Poisson sampling (*):                    773.949\\n\\nNo user-level privacy guarantee is possible without a bound on the number of\\nexamples per user.\\n\\n(*) Poisson sampling is not usually done in training pipelines, but assuming\\nthat the data was randomly shuffled, it is believed that the actual epsilon\\nshould be closer to this value than the conservative assumption of an arbitrary\\ndata order.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}